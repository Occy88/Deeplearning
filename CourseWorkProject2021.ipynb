{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5950/CS4950 Course Project\n",
    "\n",
    "This project recaps and systematises work that has been done in the exercise notebooks: if you have worked through the notebooks, little extra work is required.  The aim of the project is to give you the experience of trying to develop a neural network classifier (or regressor), involving setting up a model, optimising the amount of regularisation, investigating its performance, and devising a new model. \n",
    "\n",
    "You should do your work in this notebook, filling in the sections below. To do the work, you may re-use code from ***any of the lab-session sheets provided so far***. (In fact, you should be able to do nearly the entire project using code taken from previous lab sessions.) \n",
    "\n",
    "Please complete this workbook and submit it on Moodle, with all outputs (numbers and graphs) visible and included. \n",
    "\n",
    "The deadline for this assignment is **Wednesday April 7th, 10am**\n",
    "\n",
    "This project has **10%** of the marks of the course.\n",
    "\n",
    "Please write your student number **here**:     \n",
    "so that I have an identifier of which worksheet I am marking, to prevent any confusion! (I have to download your notebooks before marking them, so I don't want to accidentally give credit for your notebook to somebody else...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset\n",
    "\n",
    "You may choose any of the datasets included with Pytorch, or used in the course so far (including, for example, the percolation data). (It is a free country and you may use any other data you wish.)\n",
    "\n",
    "A safe choice would be either the MNIST data or the MNIST-fashion data, which is a drop-in replacement for MNIST (same size data format, same number of classes, same number of training and test examples). \n",
    "\n",
    "A more interesting choice would be CIFAR-10\n",
    "\n",
    "In setting up the data, you should set up a training set and a test set. The test set should be large enough to give a reasonably accurate assessment of the error-rate (or loss) of your models: preferably at least 10,000 examples.\n",
    "\n",
    "For the learning curve experiment (below), you will need to construct training sets of different sizes, with the largest at least 10 times the smallest. For the MNIST data, for example, your smallest training set might be 500, with sizes 500, 1000, 2000, 4000, 8000, 16000, 32000, and perhaps 60000 if you have time. (You get no extra marks for doing very long experiments.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model \n",
    "\n",
    "Set up a Pytorch model (you may find it helpful to keep it simple and fast to train). Train on an intermediate-sized dataset (if you are using MNIST, say 2000 or 4000). \n",
    "\n",
    "\n",
    "Plot the loss and error rate as a function of training epochs. \n",
    "\n",
    "\n",
    "###  1.  Assessment of initial model:    ***12 marks***\n",
    "\n",
    "Ensure that your model is complex enough to *overfit* the training data: that is the loss/error-rate on the training set should be below your target loss/error-rate, and the loss/error-rate on the validation set should be higher than this (preferably higher than your target error-rate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# your code here (you can freely used and modify code from course lab-sheets throughout this project)\n",
    "# importing cfar 10:\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# 3 channels\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "channels=3 #flattened images.\n",
    "mean=[0.5]*channels\n",
    "std=[0.5]*channels\n",
    "batch_size=32\n",
    "\n",
    "training_set_sizes=[500,1000,2000,4000,10000]\n",
    "print(\"LOADING\")\n",
    "transform=T.Compose([T.ToTensor(), T.Normalize(mean,std),T.Lambda(lambda x: torch.flatten(x))])\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinNet,self).__init__()\n",
    "        num_f=32*32*3\n",
    "        self.fc1=nn.Linear(num_f,100)\n",
    "        self.fc2=nn.Linear(100,100)\n",
    "        self.fc3=nn.Linear(100,len(classes))\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train(net,test_size,decay=None,e=5):\n",
    "    if decay:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9,weight_decay=decay)\n",
    "    else:\n",
    "        optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    acc=[]\n",
    "    av_acc=[]\n",
    "    for idx,epoch in enumerate(range(e)):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            if i>test_size:\n",
    "                break\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            out_l=outputs.max(1)[1]\n",
    "            corr=(out_l == labels).sum().item()/batch_size\n",
    "            acc.append(corr)\n",
    "            av_acc.append(np.sum(acc[max((0,len(acc)-100)):len(acc)])/min((len(acc),100)))\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "    return acc,av_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#=============== DO NOT RUN (LONGER) ==========\n",
    "from matplotlib import pyplot as plt\n",
    "cf10_train=torchvision.datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)\n",
    "classes = cf10_train.classes\n",
    "trainloader = torch.utils.data.DataLoader(cf10_train, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False, num_workers=8)\n",
    "\n",
    "print(\"====: \",training_set_sizes)\n",
    "for v in training_set_sizes:\n",
    "    net = LinNet().to(device)\n",
    "\n",
    "    print(v)\n",
    "    acc,av_acc=train(net,v)\n",
    "    plt.plot(acc)\n",
    "    plt.legend(title=f'accuracy set_size={v}')\n",
    "    plt.show()\n",
    "    plt.plot(av_acc)\n",
    "    plt.legend(title=f'av accutacy set_size={v}')\n",
    "    plt.show()\n",
    "    print('Finished Training')\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(net.state_dict(), PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#========================\n",
    "def eval(title=''):\n",
    "    net = LinNet()\n",
    "    net.load_state_dict(torch.load(PATH))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    acc_test=[]\n",
    "    av_acc_test=[]\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            corr=(predicted == labels).sum().item()\n",
    "            correct += corr\n",
    "            corr/=batch_size\n",
    "            acc_test.append(corr)\n",
    "            av_acc_test.append(np.sum(acc_test[max((0,len(acc_test)-100)):len(acc_test)])/min((len(acc_test),100)))\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "    plt.plot(av_acc)\n",
    "    plt.plot([correct / total]*len(av_acc))\n",
    "    plt.legend(title=title)\n",
    "    plt.show()\n",
    "    print(device)\n",
    "    return correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.  Regularisation experiment:  ***12 marks***\n",
    "\n",
    "Now select a type of regularisation (which may be L2, or dropout) and train your initial model with different amounts of regularisation. \n",
    "\n",
    "A small amount of regularisation may improve the validation set performance of your overfitted model; too much regularisation may make performance on the validation set worse. \n",
    "\n",
    "Train your model multiple times, applying different amounts of regularisation, and plot a graoph showing the effect of different amounts of regularisation. \n",
    "\n",
    "Plot the amount of regularisation along the x axis, and the validation set performance (loss/error-rate) on the y axis.  How does regularisation affect the performance of your initial network, and what is the optimal amount? "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reg_values=[0.0001,0.0005,0.001,0.005,0.01,0.01]\n",
    "reg_acc=[]\n",
    "for w in reg_values:\n",
    "    net = LinNet().to(device)\n",
    "\n",
    "    acc,av_acc=train(net,10000,decay=w,e=5)\n",
    "    plt.plot(acc)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(av_acc)\n",
    "    plt.legend(title=f'L2 norm, decay={w}')\n",
    "    plt.show()\n",
    "    print('Finished Training')\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    reg_acc.append(eval(f'Eval, weight decay (L2)={w}'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(range(len(reg_values)),reg_acc)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.  Learning curve experiment:  ***12 marks***\n",
    "\n",
    "Now train your model (with the amount of regularisation you selected) on different amounts of training data. \n",
    "\n",
    "Plot the performance (loss/error-rate) of the model on the validation set against the size of the training set. \n",
    "This is typically best done on a log-log plot. \n",
    "\n",
    "Describe the approximate relationship between the training set size and loss / error-rate.  Does the network performance appear to improve as some power of the amount of data in the training set?  If so, by what power ? \n",
    "\n",
    "For example, a very good rate of improvement is for error-rate to be proportional to $\\dfrac{1}{\\sqrt{n}}$ where $n$ is the training set size.  For your model, the rate of improvement of validation error with training set size may not be as fast as this (or it may, who knows?)  The aim of this exercise is to find out what it actually is. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "decay=0.001\n",
    "ts_acc=[0]\n",
    "imp_calc=[]\n",
    "act_imp=[]\n",
    "est=0\n",
    "for v in training_set_sizes:\n",
    "    print(act_imp)\n",
    "    net = LinNet().to(device)\n",
    "    est=1/sqrt(v)\n",
    "    imp_calc.append(est)\n",
    "    acc,av_acc=train(net,v,decay=decay,e=5)\n",
    "    plt.plot(av_acc)\n",
    "    plt.legend(title=f'decay={decay}, training size = {v}')\n",
    "    plt.show()\n",
    "    print('Finished Training')\n",
    "    PATH = './cifar_net.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    ts_acc.append(eval(f'decay={decay}, training size = {v}'))\n",
    "    act_imp.append(ts_acc[len(ts_acc)-1]-ts_acc[len(ts_acc)-2])\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(ts_acc)\n",
    "plt.plot(np.log(training_set_sizes),ts_acc[1:])\n",
    "plt.legend(title=f'log training size vs accuracy')\n",
    "plt.show()\n",
    "plt.plot(imp_calc,label='1/sqrt(n)')\n",
    "plt.plot(act_imp,label='actual improvement')\n",
    "plt.legend(title=f'1/sqrt(training_size) and actual impovement for training size')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Second model:   *** 14 marks ***\n",
    "\n",
    "Devise what you believe to be a better neural network architecture for the problem (e.g. for MNIST you might use a fully-connected network for your initial model, and then try a convolutional net for your second model).\n",
    "\n",
    "Repeat sections 2 (regularisation experiment), and determine whether it has better validation set performance than the first model, for an intermediate size of training set (e.g. 2000 or 4000 for the MNIST data). \n",
    "\n",
    "Repeat section 3. Does your model have a different learning curve from the first? Plot the learning curves for the first and second model on the same graph, to compare them. Comment: is there a more rapid reduction of error-rate with training set size for your second model?  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet,self).__init__()\n",
    "        num_f=32*32*3\n",
    "        nn.Conv2d(3,6,3)\n",
    "        nn.Conv2d(6,12,3)\n",
    "\n",
    "        self.fc3=nn.Linear(100,len(classes))\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}